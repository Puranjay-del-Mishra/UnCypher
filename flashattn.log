Using pip 22.0.2 from /usr/lib/python3/dist-packages/pip (python 3.10)
Defaulting to user installation because normal site-packages is not writeable
Collecting flash-attn
  Using cached flash_attn-2.7.4.post1.tar.gz (6.0 MB)
  Preparing metadata (setup.py): started
  Running command python setup.py egg_info
  /usr/lib/python3/dist-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.
    warnings.warn(


  torch.__version__  = 2.6.0+cu118


  running egg_info
  creating /tmp/pip-pip-egg-info-3w6br_vn/flash_attn.egg-info
  writing /tmp/pip-pip-egg-info-3w6br_vn/flash_attn.egg-info/PKG-INFO
  writing dependency_links to /tmp/pip-pip-egg-info-3w6br_vn/flash_attn.egg-info/dependency_links.txt
  writing requirements to /tmp/pip-pip-egg-info-3w6br_vn/flash_attn.egg-info/requires.txt
  writing top-level names to /tmp/pip-pip-egg-info-3w6br_vn/flash_attn.egg-info/top_level.txt
  writing manifest file '/tmp/pip-pip-egg-info-3w6br_vn/flash_attn.egg-info/SOURCES.txt'
  /home/pmishra/.local/lib/python3.10/site-packages/torch/utils/cpp_extension.py:529: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.
    warnings.warn(msg.format('we could not find ninja.'))
  reading manifest file '/tmp/pip-pip-egg-info-3w6br_vn/flash_attn.egg-info/SOURCES.txt'
  reading manifest template 'MANIFEST.in'
  warning: no files found matching '*.cu' under directory 'flash_attn'
  warning: no files found matching '*.h' under directory 'flash_attn'
  warning: no files found matching '*.cuh' under directory 'flash_attn'
  warning: no files found matching '*.cpp' under directory 'flash_attn'
  warning: no files found matching '*.hpp' under directory 'flash_attn'
  adding license file 'LICENSE'
  adding license file 'AUTHORS'
  writing manifest file '/tmp/pip-pip-egg-info-3w6br_vn/flash_attn.egg-info/SOURCES.txt'
  Preparing metadata (setup.py): finished with status 'done'
Collecting einops
  Using cached einops-0.8.1-py3-none-any.whl (64 kB)
Requirement already satisfied: torch in /home/pmishra/.local/lib/python3.10/site-packages (from flash-attn) (2.6.0+cu118)
Requirement already satisfied: typing-extensions>=4.10.0 in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (4.12.2)
Requirement already satisfied: fsspec in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (2024.6.1)
Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (9.1.0.70)
Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.7.5.86)
Requirement already satisfied: networkx in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (3.3)
Requirement already satisfied: jinja2 in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (3.1.6)
Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.11.3.6)
Requirement already satisfied: filelock in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (3.13.1)
Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (2.21.5)
Requirement already satisfied: sympy==1.13.1 in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (1.13.1)
Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.4.1.48)
Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (10.3.0.86)
Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.8.87)
Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.8.89)
Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.8.86)
Requirement already satisfied: triton==3.2.0 in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (3.2.0)
Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.8.89)
Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/pmishra/.local/lib/python3.10/site-packages (from torch->flash-attn) (10.9.0.58)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/pmishra/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in /home/pmishra/.local/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (3.0.2)
Building wheels for collected packages: flash-attn
  Building wheel for flash-attn (setup.py): started
  Running command python setup.py bdist_wheel


  torch.__version__  = 2.6.0+cu118


  /usr/lib/python3/dist-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.
    warnings.warn(
  running bdist_wheel
  Guessing wheel URL:  https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu11torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
  Precompiled wheel not found. Building from source...
  /home/pmishra/.local/lib/python3.10/site-packages/torch/utils/cpp_extension.py:529: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.
    warnings.warn(msg.format('we could not find ninja.'))
  running build
  running build_py
  creating build
  creating build/lib.linux-x86_64-3.10
  creating build/lib.linux-x86_64-3.10/flash_attn
  copying flash_attn/flash_attn_triton_og.py -> build/lib.linux-x86_64-3.10/flash_attn
  copying flash_attn/flash_blocksparse_attention.py -> build/lib.linux-x86_64-3.10/flash_attn
  copying flash_attn/fused_softmax.py -> build/lib.linux-x86_64-3.10/flash_attn
  copying flash_attn/flash_attn_triton.py -> build/lib.linux-x86_64-3.10/flash_attn
  copying flash_attn/bert_padding.py -> build/lib.linux-x86_64-3.10/flash_attn
  copying flash_attn/flash_blocksparse_attn_interface.py -> build/lib.linux-x86_64-3.10/flash_attn
  copying flash_attn/flash_attn_interface.py -> build/lib.linux-x86_64-3.10/flash_attn
  copying flash_attn/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn
  creating build/lib.linux-x86_64-3.10/hopper
  copying hopper/test_kvcache.py -> build/lib.linux-x86_64-3.10/hopper
  copying hopper/test_flash_attn.py -> build/lib.linux-x86_64-3.10/hopper
  copying hopper/test_util.py -> build/lib.linux-x86_64-3.10/hopper
  copying hopper/benchmark_split_kv.py -> build/lib.linux-x86_64-3.10/hopper
  copying hopper/test_attn_kvcache.py -> build/lib.linux-x86_64-3.10/hopper
  copying hopper/benchmark_flash_attention_fp8.py -> build/lib.linux-x86_64-3.10/hopper
  copying hopper/benchmark_attn.py -> build/lib.linux-x86_64-3.10/hopper
  copying hopper/generate_kernels.py -> build/lib.linux-x86_64-3.10/hopper
  copying hopper/setup.py -> build/lib.linux-x86_64-3.10/hopper
  copying hopper/flash_attn_interface.py -> build/lib.linux-x86_64-3.10/hopper
  copying hopper/__init__.py -> build/lib.linux-x86_64-3.10/hopper
  copying hopper/padding.py -> build/lib.linux-x86_64-3.10/hopper
  creating build/lib.linux-x86_64-3.10/flash_attn/models
  copying flash_attn/models/vit.py -> build/lib.linux-x86_64-3.10/flash_attn/models
  copying flash_attn/models/llama.py -> build/lib.linux-x86_64-3.10/flash_attn/models
  copying flash_attn/models/baichuan.py -> build/lib.linux-x86_64-3.10/flash_attn/models
  copying flash_attn/models/gptj.py -> build/lib.linux-x86_64-3.10/flash_attn/models
  copying flash_attn/models/bert.py -> build/lib.linux-x86_64-3.10/flash_attn/models
  copying flash_attn/models/opt.py -> build/lib.linux-x86_64-3.10/flash_attn/models
  copying flash_attn/models/btlm.py -> build/lib.linux-x86_64-3.10/flash_attn/models
  copying flash_attn/models/gpt.py -> build/lib.linux-x86_64-3.10/flash_attn/models
  copying flash_attn/models/falcon.py -> build/lib.linux-x86_64-3.10/flash_attn/models
  copying flash_attn/models/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/models
  copying flash_attn/models/bigcode.py -> build/lib.linux-x86_64-3.10/flash_attn/models
  copying flash_attn/models/gpt_neox.py -> build/lib.linux-x86_64-3.10/flash_attn/models
  creating build/lib.linux-x86_64-3.10/flash_attn/modules
  copying flash_attn/modules/embedding.py -> build/lib.linux-x86_64-3.10/flash_attn/modules
  copying flash_attn/modules/mha.py -> build/lib.linux-x86_64-3.10/flash_attn/modules
  copying flash_attn/modules/block.py -> build/lib.linux-x86_64-3.10/flash_attn/modules
  copying flash_attn/modules/mlp.py -> build/lib.linux-x86_64-3.10/flash_attn/modules
  copying flash_attn/modules/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/modules
  creating build/lib.linux-x86_64-3.10/flash_attn/ops
  copying flash_attn/ops/fused_dense.py -> build/lib.linux-x86_64-3.10/flash_attn/ops
  copying flash_attn/ops/rms_norm.py -> build/lib.linux-x86_64-3.10/flash_attn/ops
  copying flash_attn/ops/activations.py -> build/lib.linux-x86_64-3.10/flash_attn/ops
  copying flash_attn/ops/layer_norm.py -> build/lib.linux-x86_64-3.10/flash_attn/ops
  copying flash_attn/ops/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/ops
  creating build/lib.linux-x86_64-3.10/flash_attn/layers
  copying flash_attn/layers/patch_embed.py -> build/lib.linux-x86_64-3.10/flash_attn/layers
  copying flash_attn/layers/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/layers
  copying flash_attn/layers/rotary.py -> build/lib.linux-x86_64-3.10/flash_attn/layers
  creating build/lib.linux-x86_64-3.10/flash_attn/utils
  copying flash_attn/utils/pretrained.py -> build/lib.linux-x86_64-3.10/flash_attn/utils
  copying flash_attn/utils/distributed.py -> build/lib.linux-x86_64-3.10/flash_attn/utils
  copying flash_attn/utils/generation.py -> build/lib.linux-x86_64-3.10/flash_attn/utils
  copying flash_attn/utils/benchmark.py -> build/lib.linux-x86_64-3.10/flash_attn/utils
  copying flash_attn/utils/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/utils
  creating build/lib.linux-x86_64-3.10/flash_attn/losses
  copying flash_attn/losses/cross_entropy.py -> build/lib.linux-x86_64-3.10/flash_attn/losses
  copying flash_attn/losses/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/losses
  creating build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd
  copying flash_attn/flash_attn_triton_amd/utils.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd
  copying flash_attn/flash_attn_triton_amd/fwd_ref.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd
  copying flash_attn/flash_attn_triton_amd/fwd_prefill.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd
  copying flash_attn/flash_attn_triton_amd/bench.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd
  copying flash_attn/flash_attn_triton_amd/test.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd
  copying flash_attn/flash_attn_triton_amd/interface_torch.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd
  copying flash_attn/flash_attn_triton_amd/bwd_prefill.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd
  copying flash_attn/flash_attn_triton_amd/fwd_decode.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd
  copying flash_attn/flash_attn_triton_amd/bwd_ref.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd
  copying flash_attn/flash_attn_triton_amd/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd
  copying flash_attn/flash_attn_triton_amd/interface_fa.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd
  creating build/lib.linux-x86_64-3.10/flash_attn/ops/triton
  copying flash_attn/ops/triton/k_activations.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton
  copying flash_attn/ops/triton/layer_norm.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton
  copying flash_attn/ops/triton/linear.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton
  copying flash_attn/ops/triton/cross_entropy.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton
  copying flash_attn/ops/triton/mlp.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton
  copying flash_attn/ops/triton/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton
  copying flash_attn/ops/triton/rotary.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton
  running build_ext
  /home/pmishra/.local/lib/python3.10/site-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 11.8
    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')
  building 'flash_attn_2_cuda' extension
  creating build/temp.linux-x86_64-3.10
  creating build/temp.linux-x86_64-3.10/csrc
  creating build/temp.linux-x86_64-3.10/csrc/flash_attn
  creating build/temp.linux-x86_64-3.10/csrc/flash_attn/src
  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-install-hbxo_153/flash-attn_38a023bdf4dc49c0ae7d36cd9f51584b/csrc/flash_attn -I/tmp/pip-install-hbxo_153/flash-attn_38a023bdf4dc49c0ae7d36cd9f51584b/csrc/flash_attn/src -I/tmp/pip-install-hbxo_153/flash-attn_38a023bdf4dc49c0ae7d36cd9f51584b/csrc/cutlass/include -I/home/pmishra/.local/lib/python3.10/site-packages/torch/include -I/home/pmishra/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/pmishra/.local/lib/python3.10/site-packages/torch/include/TH -I/home/pmishra/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/flash_api.cpp -o build/temp.linux-x86_64-3.10/csrc/flash_attn/flash_api.o -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE="_gcc" -DPYBIND11_STDLIB="_libstdcpp" -DPYBIND11_BUILD_ABI="_cxxabi1011" -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
